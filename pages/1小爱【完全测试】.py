import os
import google.generativeai as genai
import streamlit as st
import pickle
from io import BytesIO
from PIL import Image

# ==============================================================================
# 1. æ‰€æœ‰å¸¸é‡å®šä¹‰ (Constants)
# ==============================================================================

# --- API å¯†é’¥è®¾ç½® ---
# ã€è¯·åœ¨è¿™é‡Œå¡«å…¥æ‚¨çš„API Keysã€‘
API_KEYS = {
    "04 1å·20270168962": "AIzaSyDGjLL0nJWkqDYj2KDWJdAh3zPwLPmIA_E",
    "04 2å·371111309083": "AIzaSyAQIz1pb84NzSzDwbtM2vB04zhLw8zVqdA",
    "04 3å·622662315859":"AIzaSyD9JGoGhibXPWdNmpcfmrqZ-zxpEyg67EQ",
	"04 5å·375090949096":"AIzaSyCLuGSiCE-3lxciVRiD28aBBFROMPoFU3M",
	"04 6å·340384272853":"AIzaSyDp6qJunrZ37Nc9ucbItUJB-L8lYXDdGgs",
	"04 7å·371111309083":"AIzaSyAQIz1pb84NzSzDwbtM2vB04zhLw8zVqdA",
	
    "1 1å·799638464939":"AIzaSyA5fPQo1F4qlywciLyZQknpCq0ScuOTpv4",
	
    "2 1å·866575601521":"AIzaSyD9_DVaXprbjlM6pqhbcpznrOPYib_OLcc",
	"2 3å·961280537667":"AIzaSyClCCaPvJ8uqM23fQ_ZxpbA_EqA4g1i2V8",

	"01 1å·227514221200":"AIzaSyBTyNjfDMw5tX5kOMi9i3g9OOnwDovMtQI",
	
	"02 1å·163679758614":"AIzaSyCEoSXnALUnxMSvWpK4AWYre99mxNydKZY",

	"03 1å·702122391294":"AIzaSyBQodu9EWd8VlLteNTiL0pXfSDPI_WobHI",

	"05 1å·668014237032":"AIzaSyAMR-NyB5TA2IFzJr1sgdvCqrpSBIzkkdI",
	"05 2å·851244762061":"AIzaSyAV6awcQC4NUQaX241EM72zxsGdwSEFMm0",
	
}

# --- æ¨¡å‹é…ç½® ---
MODELS = {
    "gemini-2.5-flash-preview-05-20 (é»˜è®¤)": "gemini-2.5-flash-preview-05-20",
    "gemini-2.5-pro": "gemini-2.5-pro",
}
DEFAULT_MODEL_NAME = "gemini-2.5-flash-preview-05-20 (é»˜è®¤)"

# --- æ¨¡å‹æ ¸å¿ƒé…ç½® ---
GENERATION_CONFIG = {
  "temperature": 1.0, "top_p": 0.95, "top_k": 40, "max_output_tokens": 8192,
}
SAFETY_SETTINGS = [
    {"category": c, "threshold": "BLOCK_NONE"} for c in [
        "HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", 
        "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"
    ]
]

# ==============================================================================
# 2. Session State åˆå§‹åŒ–
# ==============================================================================
if "messages" not in st.session_state:
    st.session_state.messages = []
if "is_generating" not in st.session_state:
    st.session_state.is_generating = False
if "selected_api_key" not in st.session_state:
    st.session_state.selected_api_key = list(API_KEYS.keys())[0]
if "selected_model_name" not in st.session_state:
    st.session_state.selected_model_name = DEFAULT_MODEL_NAME

# ==============================================================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ==============================================================================

# --- æ–‡ä»¶ä¸å†å²è®°å½•æ“ä½œ ---
log_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chat_history_simple.pkl")

def _prepare_messages_for_save(messages):
    picklable_messages = []
    for msg in messages:
        # ç®€åŒ–ç‰ˆï¼šåªå¤„ç†æ–‡æœ¬å’Œå›¾ç‰‡
        if msg.get("role") in ["user", "assistant"]:
            new_msg = {"role": msg["role"], "content": []}
            content = msg.get("content", [])
            for part in content:
                if isinstance(part, str):
                    new_msg["content"].append(part)
                elif isinstance(part, Image.Image):
                    buffered = BytesIO()
                    part.save(buffered, format="PNG")
                    new_msg["content"].append({"type": "image", "data": buffered.getvalue()})
            picklable_messages.append(new_msg)
    return picklable_messages

def _reconstitute_messages_after_load(messages):
    reconstituted_messages = []
    for msg in messages:
        new_msg = {"role": msg["role"], "content": []}
        for part in msg.get("content", []):
            if isinstance(part, str):
                new_msg["content"].append(part)
            elif isinstance(part, dict) and part.get("type") == "image":
                try:
                    new_msg["content"].append(Image.open(BytesIO(part["data"])))
                except Exception:
                    new_msg["content"].append("[å›¾ç‰‡åŠ è½½å¤±è´¥]")
        reconstituted_messages.append(new_msg)
    return reconstituted_messages

def load_history():
    try:
        if os.path.exists(log_file) and os.path.getsize(log_file) > 0:
            with open(log_file, "rb") as f:
                st.session_state.messages = _reconstitute_messages_after_load(pickle.load(f))
    except Exception as e:
        st.error(f"è¯»å–å†å²è®°å½•å¤±è´¥: {e}")

def save_history():
    try:
        with open(log_file, "wb") as f:
            pickle.dump(_prepare_messages_for_save(st.session_state.messages), f)
    except Exception as e:
        st.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

def clear_history():
    st.session_state.messages.clear()
    if os.path.exists(log_file):
        os.remove(log_file)
    st.toast("å†å²è®°å½•å·²æ¸…é™¤ï¼", icon="ğŸ—‘ï¸")

# --- AI ç”Ÿæˆé€»è¾‘ ---
def get_ai_response(model, history):
    """ä¸€ä¸ªç®€åŒ–çš„ç”Ÿæˆå‡½æ•°"""
    try:
        response = model.generate_content(contents=history, stream=True)
        for chunk in response:
            yield chunk.text
    except Exception as e:
        # ç›´æ¥åœ¨ç”Ÿæˆå™¨ä¸­æŠ›å‡ºé”™è¯¯ï¼Œä»¥ä¾¿ä¸»é€»è¾‘æ•è·
        yield f"**ç”Ÿæˆæ—¶å‡ºé”™ï¼š**\n\n`{type(e).__name__}: {e}`"
        # æ‰“å°å®Œæ•´çš„è¿½æº¯åˆ°æ§åˆ¶å°ï¼Œä¾›å¼€å‘è€…è°ƒè¯•
        import traceback
        traceback.print_exc()

# ==============================================================================
# 4. Streamlit ç•Œé¢æ¸²æŸ“
# ==============================================================================

st.set_page_config(page_title="æç®€èŠå¤©", layout="centered")

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("æ§åˆ¶é¢æ¿")

    # API å’Œæ¨¡å‹é€‰æ‹©
    st.selectbox(
        "é€‰æ‹©API Key:",
        options=list(API_KEYS.keys()),
        key="selected_api_key"
    )
    st.selectbox(
        "é€‰æ‹©æ¨¡å‹:",
        options=list(MODELS.keys()),
        key="selected_model_name"
    )

    # æ–‡ä»¶æ“ä½œ
    with st.expander("æ–‡ä»¶æ“ä½œ"):
        st.button("è¯»å–å†å²è®°å½• ğŸ“–", on_click=load_history, use_container_width=True)
        st.button("æ¸…é™¤å†å²è®°å½• ğŸ—‘ï¸", on_click=clear_history, use_container_width=True)
        # æä¾›ä¸€ä¸ªä¸‹è½½æŒ‰é’®ä½œä¸ºå¤‡ä»½
        if st.session_state.messages:
             st.download_button(
                 "ä¸‹è½½å½“å‰èŠå¤©è®°å½• â¬‡ï¸",
                 data=pickle.dumps(_prepare_messages_for_save(st.session_state.messages)),
                 file_name="chat_history_backup.pkl",
                 mime="application/octet-stream",
                 use_container_width=True
             )

# --- ä¸»èŠå¤©ç•Œé¢ ---
if not st.session_state.messages:
    load_history()

# æ˜¾ç¤ºèŠå¤©è®°å½•
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        for part in message.get("content", []):
            if isinstance(part, str):
                st.markdown(part)
            elif isinstance(part, Image.Image):
                st.image(part, width=400)

# æ ¸å¿ƒäº¤äº’é€»è¾‘
prompt = st.chat_input("è¾“å…¥ä½ çš„æ¶ˆæ¯...", disabled=st.session_state.is_generating)
if prompt:
    st.session_state.messages.append({"role": "user", "content": [prompt]})
    st.session_state.is_generating = True
    st.rerun() # ç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯

# æ ¸å¿ƒç”Ÿæˆé€»è¾‘
if st.session_state.is_generating:
    with st.chat_message("assistant"):
        # é…ç½®å¹¶åˆ›å»ºæ¨¡å‹å®ä¾‹
        try:
            genai.configure(api_key=API_KEYS[st.session_state.selected_api_key])
            model = genai.GenerativeModel(
                model_name=MODELS[st.session_state.selected_model_name],
                generation_config=GENERATION_CONFIG,
                safety_settings=SAFETY_SETTINGS,
                # ã€å¯é€‰ã€‘åœ¨è¿™é‡ŒåŠ å…¥æ‚¨çš„ç³»ç»ŸæŒ‡ä»¤
                # system_instruction="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"
            )

            # å‡†å¤‡APIå†å²
            api_history = []
            for msg in st.session_state.messages:
                 # API éœ€è¦çš„æ ¼å¼æ˜¯ 'model' è€Œä¸æ˜¯ 'assistant'
                 api_role = "model" if msg["role"] == "assistant" else "user"
                 api_history.append({"role": api_role, "parts": msg["content"]})
            
            # ã€æç®€ã€‘ç›´æ¥æ¸²æŸ“ï¼Œä¸ä½¿ç”¨ st.spinner
            placeholder = st.empty()
            full_response = ""
            for chunk in get_ai_response(model, api_history):
                full_response += chunk
                placeholder.markdown(full_response + "â–Œ") # æ·»åŠ é—ªçƒå…‰æ ‡
            
            placeholder.markdown(full_response)
            
            # å°†å®Œæ•´çš„å›å¤æ·»åŠ åˆ°ä¼šè¯å†å²
            st.session_state.messages.append({"role": "assistant", "content": [full_response]})
            
        except Exception as e:
            st.error(f"å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        
        finally:
            # æ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼Œéƒ½ç»“æŸç”ŸæˆçŠ¶æ€å¹¶ä¿å­˜
            st.session_state.is_generating = False
            save_history()
            st.rerun()
